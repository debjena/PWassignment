{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iGDM7q7ZGMbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **KNN & PCA**"
      ],
      "metadata": {
        "id": "owHb2A80GJQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“˜ PW Skills - KNN & PCA Assignment (Colab Ready)\n",
        "\n",
        "# ---\n",
        "# âœ… Q1. What is K-Nearest Neighbors (KNN) and how does it work?\n",
        "# KNN is a supervised algorithm used for classification & regression.\n",
        "# - Classification: Predicts the class label by majority voting among k-nearest neighbors.\n",
        "# - Regression: Predicts the value by averaging the k-nearest neighbors.\n",
        "# Distance metrics (e.g., Euclidean, Manhattan) are used to identify neighbors.\n",
        "\n",
        "# ---\n",
        "# âœ… Q2. What is the Curse of Dimensionality and how does it affect KNN?\n",
        "# Curse of Dimensionality = Problems when data has too many features.\n",
        "# - Distances lose meaning â†’ harder to find nearest neighbors.\n",
        "# - KNN becomes less effective, risk of overfitting.\n",
        "# Solution: Dimensionality reduction (e.g., PCA).\n",
        "\n",
        "# ---\n",
        "# âœ… Q3. What is PCA and how is it different from feature selection?\n",
        "# PCA = Principal Component Analysis, a dimensionality reduction technique.\n",
        "# - Creates new features (principal components) that maximize variance.\n",
        "# - Unlike feature selection, PCA transforms features rather than just selecting a subset.\n",
        "\n",
        "# ---\n",
        "# âœ… Q4. Eigenvalues & Eigenvectors in PCA\n",
        "# - Eigenvectors: Directions of maximum variance (principal components).\n",
        "# - Eigenvalues: Amount of variance explained along those directions.\n",
        "# They determine importance of each component in PCA.\n",
        "\n",
        "# ---\n",
        "# âœ… Q5. How do KNN & PCA complement each other?\n",
        "# - PCA reduces dimensions â†’ combats curse of dimensionality.\n",
        "# - KNN works better on reduced feature space â†’ faster & more accurate.\n",
        "# - Together: Robust pipeline for high-dimensional data.\n",
        "\n",
        "# ---\n",
        "# âœ… Q6. KNN on Wine Dataset (with & without scaling)\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_with_scaling = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with scaling:\", acc_with_scaling)\n",
        "\n",
        "# ---\n",
        "# âœ… Q7. PCA on Wine Dataset (Explained Variance Ratio)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# ---\n",
        "# âœ… Q8. KNN on PCA-transformed dataset (top 2 components)\n",
        "pca2 = PCA(n_components=2)\n",
        "X_pca = pca2.fit_transform(X)\n",
        "\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train_pca)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test_pca, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy on original (scaled) dataset:\", acc_with_scaling)\n",
        "print(\"Accuracy on PCA (2 components):\", acc_pca)\n",
        "\n",
        "# ---\n",
        "# âœ… Q9. KNN with different distance metrics (Euclidean vs Manhattan)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test_scaled))\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test_scaled))\n",
        "\n",
        "print(\"Euclidean Accuracy:\", acc_euclidean)\n",
        "print(\"Manhattan Accuracy:\", acc_manhattan)\n",
        "\n",
        "# ---\n",
        "# âœ… Q10. High-dimensional gene expression dataset use-case\n",
        "# Step 1: Use PCA to reduce dimensionality (retain components explaining ~95% variance).\n",
        "# Step 2: Decide #components using cumulative explained variance plot.\n",
        "# Step 3: Train KNN on reduced data.\n",
        "# Step 4: Evaluate using accuracy, precision, recall, F1, cross-validation.\n",
        "# Step 5: Business Justification:\n",
        "# - PCA reduces noise, prevents overfitting.\n",
        "# - KNN is simple & interpretable.\n",
        "# - Pipeline balances accuracy & generalization for biomedical data.\n",
        "\n",
        "# Example Code (Synthetic high-dim simulation)\n",
        "X_highdim = np.random.rand(100, 500)  # 100 samples, 500 features\n",
        "y_highdim = np.random.randint(0, 2, 100)\n",
        "\n",
        "pca_hd = PCA(n_components=50)\n",
        "X_reduced = pca_hd.fit_transform(X_highdim)\n",
        "\n",
        "X_train_hd, X_test_hd, y_train_hd, y_test_hd = train_test_split(X_reduced, y_highdim, test_size=0.3, random_state=42)\n",
        "knn_hd = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_hd.fit(X_train_hd, y_train_hd)\n",
        "\n",
        "acc_hd = accuracy_score(y_test_hd, knn_hd.predict(X_test_hd))\n",
        "print(\"Accuracy on high-dimensional synthetic dataset (PCA+KNN):\", acc_hd)\n"
      ],
      "metadata": {
        "id": "dGGBjcu9GVBp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}