{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Boosting Techniques**"
      ],
      "metadata": {
        "id": "UvP2os8tLR4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "np.random.seed(42)\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, RocCurveDisplay\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except Exception as e:\n",
        "    HAS_XGB = False\n",
        "\n",
        "# ======================================================\n",
        "# Q1: What is Boosting? How does it work?\n",
        "# ======================================================\n",
        "# Boosting is an ensemble technique that builds a strong learner by sequentially\n",
        "# training weak learners. Each new learner focuses on correcting errors of the previous ensemble\n",
        "# (by reweighting samples or fitting residuals). Final prediction is a weighted combination\n",
        "# of all learners. Boosting reduces bias and can produce very accurate models.\n",
        "\n",
        "# ======================================================\n",
        "# Q2: What is AdaBoost and how does it work?\n",
        "# ======================================================\n",
        "# AdaBoost (Adaptive Boosting) trains weak learners sequentially (often decision stumps).\n",
        "# After each round, it increases weights for misclassified samples so the next learner\n",
        "# focuses on hard examples. Final prediction is a weighted vote of the learners.\n",
        "# Key hyperparameters: number of estimators, learning_rate, base_estimator complexity.\n",
        "\n",
        "# ======================================================\n",
        "# Q3: What is Gradient Boosting and how is it different from AdaBoost?\n",
        "# ======================================================\n",
        "# Gradient Boosting builds learners to fit the negative gradient (residuals) of a loss function.\n",
        "# Each new model learns to predict residuals of the previous ensemble. It is a functional\n",
        "# gradient descent approach. Differences: AdaBoost reweights samples; Gradient Boosting\n",
        "# fits residuals directly and generalizes to many loss functions (regression, classification, ranking).\n",
        "\n",
        "# ======================================================\n",
        "# Q4: XGBoost / LightGBM — benefits and why used\n",
        "# ======================================================\n",
        "# XGBoost, LightGBM (and CatBoost) are optimized gradient boosting frameworks.\n",
        "# Benefits:\n",
        "# - Speed and scalability (histogram-based splits, multi-threading)\n",
        "# - Regularization options (L1/L2, shrinkage)\n",
        "# - Built-in handling for missing values\n",
        "# - Advanced features: tree pruning, approximate algorithms, early stopping\n",
        "# Use them when you need top-of-the-line GB performance on tabular data.\n",
        "\n",
        "# ======================================================\n",
        "# Q5: When to use boosting & pitfalls\n",
        "# ======================================================\n",
        "# Use boosting when:\n",
        "# - You want high predictive accuracy on tabular data\n",
        "# - You can tolerate longer training time than simple models\n",
        "# Pitfalls:\n",
        "# - Can overfit if not regularized\n",
        "# - Sensitive to noisy labels/outliers (but regularization + early stopping helps)\n",
        "# - Less interpretable than a single decision tree (but feature importance + SHAP help)\n",
        "\n",
        "# ======================================================\n",
        "# Q6: AdaBoost on Iris — compare with base decision stump\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\"*8 + \" Q6: AdaBoost on Iris \" + \"=\"*8)\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Base learner = decision stump\n",
        "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "stump.fit(Xtr, ytr)\n",
        "pred_stump = stump.predict(Xte)\n",
        "acc_stump = accuracy_score(yte, pred_stump)\n",
        "\n",
        "# AdaBoost\n",
        "adb = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                         n_estimators=100, learning_rate=1.0, random_state=42)\n",
        "adb.fit(Xtr, ytr)\n",
        "pred_adb = adb.predict(Xte)\n",
        "acc_adb = accuracy_score(yte, pred_adb)\n",
        "\n",
        "print(f\"Decision stump accuracy: {acc_stump:.4f}\")\n",
        "print(f\"AdaBoost accuracy: {acc_adb:.4f}\")\n",
        "\n",
        "# Show classification report for AdaBoost\n",
        "print(\"\\nAdaBoost classification report:\\n\", classification_report(yte, pred_adb, target_names=iris.target_names))\n",
        "\n",
        "# Plot feature importances (AdaBoost)\n",
        "if hasattr(adb, \"feature_importances_\"):\n",
        "    fi = adb.feature_importances_\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.barh(iris.feature_names, fi)\n",
        "    plt.title(\"AdaBoost Feature Importances (Iris)\")\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.show()\n",
        "\n",
        "# ======================================================\n",
        "# Q7: Gradient Boosting on Breast Cancer dataset (classification report)\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\"*8 + \" Q7: Gradient Boosting on Breast Cancer \" + \"=\"*8)\n",
        "bc = datasets.load_breast_cancer()\n",
        "Xb, yb = bc.data, bc.target\n",
        "Xb_tr, Xb_te, yb_tr, yb_te = train_test_split(Xb, yb, test_size=0.3, random_state=42, stratify=yb)\n",
        "\n",
        "# Standard scaling helps gradient-based models\n",
        "pipe_gb = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"gb\", GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=42))\n",
        "])\n",
        "pipe_gb.fit(Xb_tr, yb_tr)\n",
        "pred_gb = pipe_gb.predict(Xb_te)\n",
        "acc_gb = accuracy_score(yb_te, pred_gb)\n",
        "print(f\"Gradient Boosting accuracy: {acc_gb:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(yb_te, pred_gb, target_names=bc.target_names))\n",
        "\n",
        "# Plot feature importances\n",
        "gb = pipe_gb.named_steps[\"gb\"]\n",
        "fi_gb = pd.Series(gb.feature_importances_, index=bc.feature_names).nlargest(10)\n",
        "plt.figure(figsize=(6,4))\n",
        "fi_gb.plot.barh()\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Top 10 Feature Importances (GradientBoosting on Breast Cancer)\")\n",
        "plt.show()\n",
        "\n",
        "# ======================================================\n",
        "# Q8: XGBoost on Wine dataset — GridSearchCV for C-like parameters (if XGBoost available)\n",
        "# If XGBoost not available, use HistGradientBoostingClassifier as a modern alternative.\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\"*8 + \" Q8: XGBoost (or HGB) on Wine with GridSearchCV \" + \"=\"*8)\n",
        "wine = datasets.load_wine()\n",
        "Xw, yw = wine.data, wine.target\n",
        "Xw_tr, Xw_te, yw_tr, yw_te = train_test_split(Xw, yw, test_size=0.3, random_state=42, stratify=yw)\n",
        "\n",
        "if HAS_XGB:\n",
        "    print(\"XGBoost is available. Running GridSearchCV on XGBClassifier (may take some time)...\")\n",
        "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, verbosity=0)\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [50, 100],\n",
        "        \"max_depth\": [3, 5],\n",
        "        \"learning_rate\": [0.01, 0.1]\n",
        "    }\n",
        "    grid_xgb = GridSearchCV(xgb_model, param_grid, cv=4, n_jobs=-1)\n",
        "    grid_xgb.fit(Xw_tr, yw_tr)\n",
        "    best = grid_xgb.best_estimator_\n",
        "    print(\"Best XGBoost params:\", grid_xgb.best_params_)\n",
        "    print(\"CV best score:\", grid_xgb.best_score_)\n",
        "    pred_xgb = best.predict(Xw_te)\n",
        "    print(\"Test accuracy (best XGBoost):\", accuracy_score(yw_te, pred_xgb))\n",
        "    # Feature importances\n",
        "    try:\n",
        "        fi = pd.Series(best.feature_importances_, index=wine.feature_names).nlargest(10)\n",
        "        plt.figure(figsize=(6,4)); fi.plot.barh(); plt.gca().invert_yaxis(); plt.title(\"XGBoost Feature Importances\"); plt.show()\n",
        "    except Exception:\n",
        "        pass\n",
        "else:\n",
        "    print(\"XGBoost not found — using HistGradientBoostingClassifier (sklearn) as alternative.\")\n",
        "    hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "    param_grid = {\"max_iter\": [100, 200], \"max_depth\": [None, 10]}\n",
        "    grid_hgb = GridSearchCV(hgb, param_grid, cv=4, n_jobs=-1)\n",
        "    grid_hgb.fit(Xw_tr, yw_tr)\n",
        "    best = grid_hgb.best_estimator_\n",
        "    print(\"Best HGB params:\", grid_hgb.best_params_)\n",
        "    print(\"CV best score:\", grid_hgb.best_score_)\n",
        "    pred_hgb = best.predict(Xw_te)\n",
        "    print(\"Test accuracy (best HGB):\", accuracy_score(yw_te, pred_hgb))\n",
        "    # Feature importances\n",
        "    try:\n",
        "        fi = pd.Series(best.feature_importances_, index=wine.feature_names).nlargest(10)\n",
        "        plt.figure(figsize=(6,4)); fi.plot.barh(); plt.gca().invert_yaxis(); plt.title(\"HGB Feature Importances\"); plt.show()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ======================================================\n",
        "# Q9: Feature importance comparison & a simple partial-dependence style plot\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\"*8 + \" Q9: Feature importances & PD-style plot \" + \"=\"*8)\n",
        "# Use RandomForest or best gradient model's importances if available; fallback to AdaBoost (from Q6)\n",
        "models_for_fi = {}\n",
        "models_for_fi[\"AdaBoost(Iris)\"] = adb if 'adb' in globals() else None\n",
        "models_for_fi[\"GradientBoosting(BreastCancer)\"] = gb if 'gb' in globals() else None\n",
        "if HAS_XGB and 'best' in globals():\n",
        "    models_for_fi[\"XGBoost/Winner\"] = best\n",
        "\n",
        "# Print available importances\n",
        "for name, model in models_for_fi.items():\n",
        "    if model is None:\n",
        "        continue\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        try:\n",
        "            importances = model.feature_importances_\n",
        "            print(f\"\\n{name} top features:\")\n",
        "            if name.startswith(\"AdaBoost\"):\n",
        "                cols = iris.feature_names\n",
        "            elif name.startswith(\"GradientBoosting\"):\n",
        "                cols = bc.feature_names\n",
        "            elif name.startswith(\"XGBoost\"):\n",
        "                cols = wine.feature_names\n",
        "            else:\n",
        "                cols = [f\"f{i}\" for i in range(len(importances))]\n",
        "            fi_series = pd.Series(importances, index=cols).nlargest(5)\n",
        "            print(fi_series)\n",
        "            # Plot top 5\n",
        "            plt.figure(figsize=(5,2)); fi_series.plot.barh(); plt.gca().invert_yaxis(); plt.title(f\"{name} - Top features\"); plt.show()\n",
        "        except Exception as e:\n",
        "            print(\"Could not extract importances for\", name, \":\", e)\n",
        "\n",
        "# Simple PD-like plot: vary a single feature and show model prediction (for AdaBoost on Iris)\n",
        "try:\n",
        "    feat_idx = 2  # petal length for iris\n",
        "    grid_vals = np.linspace(X[:, feat_idx].min(), X[:, feat_idx].max(), 50)\n",
        "    X_sample = Xte.copy() if 'Xte' in globals() else X.copy()\n",
        "    preds = []\n",
        "    for v in grid_vals:\n",
        "        X_tmp = X_sample.copy()\n",
        "        X_tmp[:, feat_idx] = v\n",
        "        preds.append(adb.predict(X_tmp).mean())  # mean predicted label value (not strict PDP, but illustrative)\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.plot(grid_vals, preds)\n",
        "    plt.title(\"PD-style plot (AdaBoost on Iris) — avg predicted label vs feature value\")\n",
        "    plt.xlabel(iris.feature_names[feat_idx])\n",
        "    plt.ylabel(\"Average predicted label\")\n",
        "    plt.show()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ======================================================\n",
        "# Q10: Handling class imbalance & early stopping (practical demo)\n",
        "# - Show: (a) class weight or sample_weight usage, (b) early stopping with XGBoost/HGB\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\"*8 + \" Q10: Class imbalance & Early Stopping \" + \"=\"*8)\n",
        "# Create an imbalanced version of the breast cancer dataset by downsampling minority class\n",
        "Xb_df = pd.DataFrame(Xb, columns=bc.feature_names)\n",
        "yb_ser = pd.Series(yb)\n",
        "# original class distribution\n",
        "print(\"Original class counts:\\n\", yb_ser.value_counts())\n",
        "\n",
        "# Downsample class 1 (malignant) to create imbalance (if both classes present)\n",
        "from sklearn.utils import resample\n",
        "df = Xb_df.copy(); df['target'] = yb_ser\n",
        "# keep all class 0, downsample class 1\n",
        "df_major = df[df['target']==0]\n",
        "df_minor = df[df['target']==1]\n",
        "df_minor_down = resample(df_minor, replace=False, n_samples=int(len(df_major)*0.2), random_state=42)\n",
        "df_imbal = pd.concat([df_major, df_minor_down])\n",
        "print(\"Imbalanced class counts:\\n\", df_imbal['target'].value_counts())\n",
        "\n",
        "X_imb = df_imbal.drop(columns='target').values\n",
        "y_imb = df_imbal['target'].values\n",
        "Xtr_imb, Xte_imb, ytr_imb, yte_imb = train_test_split(X_imb, y_imb, test_size=0.3, random_state=42, stratify=y_imb)\n",
        "\n",
        "# (a) Use class_weight in base estimator / sample_weight\n",
        "# Example: AdaBoost accepts sample_weight in fit; we can pass higher weights to minority class\n",
        "weights = np.where(ytr_imb==1, 5, 1)  # give minority class more weight\n",
        "adb_imb = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, random_state=42)\n",
        "adb_imb.fit(Xtr_imb, ytr_imb, sample_weight=weights)\n",
        "pred_imb = adb_imb.predict(Xte_imb)\n",
        "print(\"\\nAdaBoost on imbalanced data (with sample_weight) accuracy:\", accuracy_score(yte_imb, pred_imb))\n",
        "print(\"Classification report:\\n\", classification_report(yte_imb, pred_imb))\n",
        "\n",
        "# (b) Early stopping demonstration with XGBoost (if available) or HistGradientBoostingClassifier\n",
        "if HAS_XGB:\n",
        "    print(\"\\nDemonstrating early stopping with XGBoost...\")\n",
        "    dtrain = xgb.DMatrix(Xtr_imb, label=ytr_imb)\n",
        "    dtest = xgb.DMatrix(Xte_imb, label=yte_imb)\n",
        "    params = {\"objective\":\"binary:logistic\", \"eval_metric\":\"logloss\", \"verbosity\":0}\n",
        "    # use a small number of rounds and early_stopping_rounds\n",
        "    watchlist = [(dtrain, \"train\"), (dtest, \"eval\")]\n",
        "    bst = xgb.train(params, dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=10, verbose_eval=False)\n",
        "    preds_prob = bst.predict(dtest)\n",
        "    preds = (preds_prob > 0.5).astype(int)\n",
        "    print(\"XGBoost with early stopping accuracy:\", accuracy_score(yte_imb, preds))\n",
        "else:\n",
        "    print(\"\\nXGBoost not installed; using HistGradientBoostingClassifier with early stopping instead.\")\n",
        "    hgb_es = HistGradientBoostingClassifier(max_iter=1000, early_stopping=True, random_state=42)\n",
        "    hgb_es.fit(Xtr_imb, ytr_imb)\n",
        "    preds = hgb_es.predict(Xte_imb)\n",
        "    print(\"HGB with early stopping accuracy:\", accuracy_score(yte_imb, preds))\n",
        "\n"
      ],
      "metadata": {
        "id": "Hy0vhhscLThL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}