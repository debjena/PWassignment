{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yf-BxdUHEh5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---\n",
        "# ✅ Q1. What is a Decision Tree, and how does it work in classification?\n",
        "# A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "# It works by splitting the dataset into branches based on feature values.\n",
        "# In classification, it asks yes/no questions to predict a class label.\n",
        "# Each internal node represents a decision based on a feature,\n",
        "# branches represent the outcomes, and leaves represent final class labels.\n",
        "\n",
        "# ---\n",
        "# ✅ Q2. Gini Impurity and Entropy as impurity measures\n",
        "# Gini Impurity:\n",
        "# Measures the likelihood of an incorrect classification.\n",
        "# Gini = 1 - sum(p_i^2), where p_i is the probability of class i\n",
        "# Entropy:\n",
        "# Measures the amount of disorder or uncertainty.\n",
        "# Entropy = -sum(p_i * log2(p_i))\n",
        "# Lower impurity (Gini or Entropy) means better splits in the Decision Tree.\n",
        "\n",
        "# ---\n",
        "# ✅ Q3. Pre-Pruning vs Post-Pruning\n",
        "# Pre-Pruning:\n",
        "# Stops tree growth early (e.g., using max_depth, min_samples_split).\n",
        "# Advantage: Prevents overfitting early.\n",
        "# Post-Pruning:\n",
        "# Tree is fully grown then pruned back by removing low-importance nodes.\n",
        "# Advantage: Can result in a simpler, more general model.\n",
        "\n",
        "# ---\n",
        "# ✅ Q4. What is Information Gain?\n",
        "# Information Gain = Reduction in entropy due to a split.\n",
        "# It helps select the best feature to split the data.\n",
        "# Higher information gain → better feature for decision.\n",
        "\n",
        "# ---\n",
        "# ✅ Q5. Real-world applications of Decision Trees\n",
        "# Applications:\n",
        "# - Medical diagnosis\n",
        "# - Credit scoring\n",
        "# - Fraud detection\n",
        "# - Marketing predictions\n",
        "# Advantages:\n",
        "# - Easy to interpret\n",
        "# - Handles both numerical and categorical data\n",
        "# Limitations:\n",
        "# - Prone to overfitting\n",
        "# - Can create biased trees if class is imbalanced\n",
        "\n",
        "# ---\n",
        "# ✅ Q6. Train Decision Tree Classifier with Gini (Iris Dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "importances = clf.feature_importances_\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", importances)\n",
        "\n",
        "# ---\n",
        "# ✅ Q7. Compare max_depth=3 vs fully grown tree\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "acc_full = clf_full.score(X_test, y_test)\n",
        "acc_limited = clf_limited.score(X_test, y_test)\n",
        "print(\"Full Tree Accuracy:\", acc_full)\n",
        "print(\"Max Depth=3 Accuracy:\", acc_limited)\n",
        "\n",
        "# ---\n",
        "# ✅ Q8. Decision Tree Regressor on Boston Housing\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "boston = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "preds = reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, preds)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n",
        "\n",
        "# ---\n",
        "# ✅ Q9. Tune max_depth and min_samples_split using GridSearchCV (Iris)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'max_depth': [2, 3, 4, 5], 'min_samples_split': [2, 3, 4, 5]}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid=params, cv=5)\n",
        "grid.fit(iris.data, iris.target)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "\n",
        "# ---\n",
        "# ✅ Q10. Healthcare use-case with Decision Tree (Step-by-step)\n",
        "# Step 1: Handle Missing Values\n",
        "# - Use imputation (mean for numerical, mode for categorical)\n",
        "# Step 2: Encode Categorical Variables\n",
        "# - Use OneHotEncoder or LabelEncoder\n",
        "# Step 3: Train the Model\n",
        "# - Use DecisionTreeClassifier with train_test_split\n",
        "# Step 4: Hyperparameter Tuning\n",
        "# - Use GridSearchCV for max_depth, min_samples_split\n",
        "# Step 5: Evaluate Model\n",
        "# - Use accuracy, confusion matrix, classification report\n",
        "# Business Value:\n",
        "# - Predict diseases early → personalized treatment\n",
        "# - Reduce costs & improve patient outcomes\n",
        "# - Enables scalable diagnosis support tools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRkdSia5Egdz",
        "outputId": "a6683299-5009-4e50-d56c-fdda28f04120"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.03334028 0.         0.88947325 0.07718647]\n",
            "Full Tree Accuracy: 1.0\n",
            "Max Depth=3 Accuracy: 1.0\n",
            "MSE: 0.4965269504982073\n",
            "Feature Importances: [0.52788015 0.05272698 0.0524354  0.02789796 0.02975409 0.13132227\n",
            " 0.09456355 0.0834196 ]\n",
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 4}\n",
            "Best Accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    }
  ]
}