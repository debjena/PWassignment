{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvcbW8gZJ5nR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cryptocurrency Volatility\n",
        "Prediction\n",
        "**"
      ],
      "metadata": {
        "id": "aM2bBvgoJ7Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cryptocurrency Volatility Prediction\n",
        "# Colab-ready notebook for the project prompt you provided.\n",
        "# Assumes dataset.zip is uploaded to Colab at /mnt/data/dataset.csv.zip\n",
        "# If you run on your local machine, change paths accordingly.\n",
        "\n",
        "# ----\n",
        "# 0. Install (optional) and imports\n",
        "# ----\n",
        "# Uncomment if running in an environment missing libraries\n",
        "# !pip install xgboost lightgbm catboost streamlit --quiet\n",
        "\n",
        "import os, zipfile, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "sns.set_style(\"whitegrid\")\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.linear_model import Ridge, LogisticRegression\n",
        "\n",
        "# Try optional advanced models\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    from xgboost import XGBRegressor, XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except:\n",
        "    HAS_XGB = False\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    HAS_LGB = True\n",
        "except:\n",
        "    HAS_LGB = False\n",
        "\n",
        "# ----\n",
        "# 1. Load dataset from uploaded zip (path: /mnt/data/dataset.csv.zip)\n",
        "# ----\n",
        "zip_path = \"/mnt/data/dataset.csv.zip\"\n",
        "if not os.path.exists(zip_path):\n",
        "    raise FileNotFoundError(f\"{zip_path} not found. Upload dataset.csv.zip to /mnt/data/\")\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    # assume single file dataset.csv inside\n",
        "    csv_names = [n for n in z.namelist() if n.lower().endswith('.csv')]\n",
        "    if not csv_names:\n",
        "        raise FileNotFoundError(\"No CSV found inside the uploaded zip.\")\n",
        "    csv_name = csv_names[0]\n",
        "    print(\"Found CSV inside zip:\", csv_name)\n",
        "    with z.open(csv_name) as f:\n",
        "        df = pd.read_csv(f)\n",
        "\n",
        "print(\"Raw data shape:\", df.shape)\n",
        "df.head()\n",
        "\n",
        "# ----\n",
        "# 2. Quick schema cleanup and type conversions\n",
        "# ----\n",
        "# Rename common columns if necessary and parse dates\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "# Ensure date column exists\n",
        "date_col = None\n",
        "for candidate in ['date', 'timestamp', 'time']:\n",
        "    if candidate in df.columns:\n",
        "        date_col = candidate\n",
        "        break\n",
        "if date_col is None:\n",
        "    raise ValueError(\"No date/timestamp column found in dataset. Expecting 'date' or 'timestamp'.\")\n",
        "\n",
        "# Parse date and sort\n",
        "df['date'] = pd.to_datetime(df[date_col])\n",
        "df = df.sort_values(['crypto_name', 'date']).reset_index(drop=True)\n",
        "\n",
        "# Basic columns we expect: open, high, low, close, volume, marketCap, crypto_name\n",
        "expected_cols = ['open','high','low','close','volume','marketCap','crypto_name']\n",
        "for c in expected_cols:\n",
        "    if c not in df.columns:\n",
        "        print(f\"Warning: expected column '{c}' not found. Check dataset. Available columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Check NA counts\n",
        "print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
        "\n",
        "# ----\n",
        "# 3. Basic EDA (per-crypto summary)\n",
        "# ----\n",
        "print(\"\\nCryptocurrencies in dataset:\", df['crypto_name'].nunique())\n",
        "print(df['crypto_name'].value_counts().head(10))\n",
        "\n",
        "# Example: plot price time series for top N cryptos by records\n",
        "top_n = df['crypto_name'].value_counts().head(5).index.tolist()\n",
        "plt.figure(figsize=(12,6))\n",
        "for name in top_n:\n",
        "    sub = df[df['crypto_name']==name]\n",
        "    plt.plot(sub['date'], sub['close'], label=name)\n",
        "plt.legend(); plt.title(\"Close price for top 5 cryptos\"); plt.xlabel(\"Date\"); plt.ylabel(\"Close\"); plt.show()\n",
        "\n",
        "# ----\n",
        "# 4. Preprocessing\n",
        "#    - Handle missing values\n",
        "#    - Fill forward/backward per crypto for OHLC and volume/mcap\n",
        "# ----\n",
        "# We'll forward-fill and then drop remaining NAs\n",
        "df.sort_values(['crypto_name','date'], inplace=True)\n",
        "df[['open','high','low','close','volume','marketCap']] = df.groupby('crypto_name')[['open','high','low','close','volume','marketCap']].apply(lambda g: g.fillna(method='ffill').fillna(method='bfill'))\n",
        "# After that, drop rows still having NaN in price\n",
        "df = df.dropna(subset=['open','high','low','close'])\n",
        "print(\"\\nAfter imputation, remaining NA counts:\\n\", df.isna().sum())\n",
        "\n",
        "# ----\n",
        "# 5. Feature engineering (per crypto)\n",
        "#    Features:\n",
        "#      - log_return\n",
        "#      - rolling volatility (std of log returns) for windows: 7, 14, 30\n",
        "#      - ATR-like measure: True Range and rolling average (ATR)\n",
        "#      - Bollinger Bands %B or bandwidth\n",
        "#      - liquidity ratio: volume / marketCap\n",
        "#      - moving averages (ma7, ma21)\n",
        "# ----\n",
        "def add_features(g):\n",
        "    g = g.copy()\n",
        "    g['log_close'] = np.log(g['close'].replace(0, np.nan))\n",
        "    g['log_return'] = g['log_close'].diff()\n",
        "    # True range\n",
        "    g['tr1'] = g['high'] - g['low']\n",
        "    g['tr2'] = (g['high'] - g['close'].shift()).abs()\n",
        "    g['tr3'] = (g['low'] - g['close'].shift()).abs()\n",
        "    g['true_range'] = g[['tr1','tr2','tr3']].max(axis=1)\n",
        "    # ATR like\n",
        "    g['ATR_14'] = g['true_range'].rolling(14, min_periods=1).mean()\n",
        "    # rolling vol\n",
        "    for w in [7,14,30]:\n",
        "        g[f'roll_vol_{w}'] = g['log_return'].rolling(window=w, min_periods=1).std()\n",
        "        g[f'ma_{w}'] = g['close'].rolling(window=w, min_periods=1).mean()\n",
        "    # Bollinger bands (20)\n",
        "    g['ma_20'] = g['close'].rolling(20, min_periods=1).mean()\n",
        "    g['std_20'] = g['close'].rolling(20, min_periods=1).std()\n",
        "    g['bb_upper'] = g['ma_20'] + 2*g['std_20']\n",
        "    g['bb_lower'] = g['ma_20'] - 2*g['std_20']\n",
        "    g['bb_width'] = (g['bb_upper'] - g['bb_lower']) / g['ma_20']\n",
        "    # liquidity\n",
        "    g['liquidity'] = g['volume'] / (g['marketCap'].replace({0:np.nan}))\n",
        "    # fill any infinite or large values\n",
        "    g.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    return g\n",
        "\n",
        "df_fe = df.groupby('crypto_name').apply(add_features).reset_index(drop=True)\n",
        "print(\"\\nAfter feature engineering shape:\", df_fe.shape)\n",
        "df_fe[[ 'crypto_name','date','close','log_return','roll_vol_7','roll_vol_14','roll_vol_30','ATR_14','bb_width','liquidity']].head(10)\n",
        "\n",
        "# ----\n",
        "# 6. Target construction\n",
        "#   Option A: Regression → predict next-day volatility (e.g., roll_vol_7 shifted -1)\n",
        "#   Option B: Classification → bin volatility into levels (low/medium/high) using quantiles\n",
        "# ----\n",
        "# We'll create both: a numeric target 'target_vol_7_next' and a categorical 'vol_level'\n",
        "df_fe['target_vol_7_next'] = df_fe.groupby('crypto_name')['roll_vol_7'].shift(-1)  # next day's 7-day rolling vol\n",
        "\n",
        "# For classification, compute per-crypto quantiles (to account for different base vol levels)\n",
        "def assign_levels(g):\n",
        "    g = g.copy()\n",
        "    q_low = g['roll_vol_7'].quantile(0.33)\n",
        "    q_high = g['roll_vol_7'].quantile(0.66)\n",
        "    def lvl(x):\n",
        "        if pd.isna(x): return np.nan\n",
        "        if x <= q_low: return 0  # low\n",
        "        elif x <= q_high: return 1  # medium\n",
        "        else: return 2  # high\n",
        "    g['vol_level'] = g['roll_vol_7'].apply(lvl)\n",
        "    return g\n",
        "\n",
        "df_fe = df_fe.groupby('crypto_name').apply(assign_levels).reset_index(drop=True)\n",
        "print(\"\\nDistribution of levels (sample):\")\n",
        "print(df_fe['vol_level'].value_counts(dropna=True))\n",
        "\n",
        "# Drop rows where target is NaN (end of series)\n",
        "df_model = df_fe.dropna(subset=['target_vol_7_next']).copy()\n",
        "\n",
        "# ----\n",
        "# 7. Choose one cryptocurrency (or build cross-crypto model)\n",
        "#    For demonstration, we'll build two workflows:\n",
        "#      (A) Per-crypto model for Bitcoin (if present)\n",
        "#      (B) Cross-crypto model (includes crypto_name as categorical via one-hot or label encoding)\n",
        "# ----\n",
        "cryptos = df_model['crypto_name'].unique().tolist()\n",
        "print(\"Available cryptos:\", cryptos[:10])\n",
        "# If Bitcoin present, pick it; else pick the largest by records\n",
        "target_crypto = 'Bitcoin' if 'Bitcoin' in cryptos else cryptos[0]\n",
        "print(\"Selected crypto for per-crypto demo:\", target_crypto)\n",
        "\n",
        "df_btc = df_model[df_model['crypto_name']==target_crypto].copy()\n",
        "print(\"Per-crypto shape:\", df_btc.shape)\n",
        "\n",
        "# ----\n",
        "# 8A. Regression (predict next-day volatility) — per-crypto example (Bitcoin)\n",
        "# Features to use: roll_vol_7, roll_vol_14, roll_vol_30, ATR_14, bb_width, liquidity, ma_7, ma_14, ma_30\n",
        "features = ['roll_vol_7','roll_vol_14','roll_vol_30','ATR_14','bb_width','liquidity','ma_7','ma_14','ma_30']\n",
        "df_btc = df_btc.dropna(subset=features + ['target_vol_7_next'])\n",
        "X = df_btc[features].values\n",
        "y = df_btc['target_vol_7_next'].values\n",
        "\n",
        "# Train-test split time-based: use last 20% as test\n",
        "split_idx = int(len(df_btc)*0.8)\n",
        "X_tr, X_te = X[:split_idx], X[split_idx:]\n",
        "y_tr, y_te = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_tr_s = scaler.fit_transform(X_tr)\n",
        "X_te_s = scaler.transform(X_te)\n",
        "\n",
        "# Baseline model: RandomForestRegressor\n",
        "rfr = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rfr.fit(X_tr_s, y_tr)\n",
        "pred_te = rfr.predict(X_te_s)\n",
        "print(\"\\nRegression (per-crypto) metrics — RandomForestRegressor:\")\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(y_te, pred_te)))\n",
        "print(\"MAE:\", mean_absolute_error(y_te, pred_te))\n",
        "print(\"R2:\", r2_score(y_te, pred_te))\n",
        "\n",
        "# Feature importances\n",
        "fi = pd.Series(rfr.feature_importances_, index=features).sort_values(ascending=False)\n",
        "print(\"\\nFeature importances:\\n\", fi)\n",
        "\n",
        "# ----\n",
        "# 8B. Classification (predict vol_level) — cross-crypto example\n",
        "# We'll build a cross-crypto model to predict current vol level (0,1,2) using features\n",
        "# Prepare dataset: drop rows with NaN vol_level\n",
        "df_class = df_model.dropna(subset=features + ['vol_level']).copy()\n",
        "\n",
        "# Simple encoding: label encode crypto_name (or drop it to keep purely market features)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df_class['crypto_label'] = le.fit_transform(df_class['crypto_name'])\n",
        "\n",
        "# Choose features + crypto_label\n",
        "class_features = features + ['crypto_label']\n",
        "Xc = df_class[class_features].fillna(0).values\n",
        "yc = df_class['vol_level'].astype(int).values\n",
        "\n",
        "# train-test split (stratified)\n",
        "Xc_tr, Xc_te, yc_tr, yc_te = train_test_split(Xc, yc, test_size=0.2, random_state=42, stratify=yc)\n",
        "\n",
        "scaler_c = StandardScaler()\n",
        "Xc_tr_s = scaler_c.fit_transform(Xc_tr)\n",
        "Xc_te_s = scaler_c.transform(Xc_te)\n",
        "\n",
        "# Baseline classifier\n",
        "rfc = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)\n",
        "rfc.fit(Xc_tr_s, yc_tr)\n",
        "yc_pred = rfc.predict(Xc_te_s)\n",
        "\n",
        "print(\"\\nClassification metrics — RandomForestClassifier (cross-crypto):\")\n",
        "print(\"Accuracy:\", accuracy_score(yc_te, yc_pred))\n",
        "print(\"Precision (macro):\", precision_score(yc_te, yc_pred, average='macro'))\n",
        "print(\"Recall (macro):\", recall_score(yc_te, yc_pred, average='macro'))\n",
        "print(\"\\nClassification report:\\n\", classification_report(yc_te, yc_pred))\n",
        "\n",
        "# Confusion matrix plot\n",
        "cm = confusion_matrix(yc_te, yc_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix (vol_level)\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.show()\n",
        "\n",
        "# ----\n",
        "# 9. Hyperparameter tuning (example grid for RandomForest regression)\n",
        "# ----\n",
        "print(\"\\nHyperparameter tuning example (RandomForestRegressor on BTC):\")\n",
        "param_grid = {\n",
        "    'n_estimators': [50,100],\n",
        "    'max_depth': [5,10,None],\n",
        "    'min_samples_leaf': [1,3]\n",
        "}\n",
        "grid = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid.fit(X_tr_s, y_tr)\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "best_rfr = grid.best_estimator_\n",
        "pred_best = best_rfr.predict(X_te_s)\n",
        "print(\"Tuned RMSE:\", np.sqrt(mean_squared_error(y_te, pred_best)))\n",
        "\n",
        "# ----\n",
        "# 10. Save model and scaler (for deployment)\n",
        "# ----\n",
        "import joblib\n",
        "os.makedirs('models', exist_ok=True)\n",
        "joblib.dump(scaler, 'models/btc_scaler.joblib')\n",
        "joblib.dump(best_rfr, 'models/btc_rfr_best.joblib')\n",
        "print(\"Saved scaler and model to models/ directory.\")\n",
        "\n",
        "# ----\n",
        "# 11. Simple Streamlit app stub (for local deploy) — save as app.py\n",
        "# ----\n",
        "streamlit_app = r'''\n",
        "# Save this as app.py and run: streamlit run app.py\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "\n",
        "scaler = joblib.load(\"models/btc_scaler.joblib\")\n",
        "model = joblib.load(\"models/btc_rfr_best.joblib\")\n",
        "\n",
        "st.title(\"Crypto Volatility Predictor (BTC example)\")\n",
        "st.markdown(\"Provide feature values to predict next-day 7-day rolling volatility.\")\n",
        "\n",
        "# Collect features\n",
        "features = [\"roll_vol_7\",\"roll_vol_14\",\"roll_vol_30\",\"ATR_14\",\"bb_width\",\"liquidity\",\"ma_7\",\"ma_14\",\"ma_30\"]\n",
        "vals = []\n",
        "for f in features:\n",
        "    vals.append(st.number_input(f, value=0.0))\n",
        "\n",
        "if st.button(\"Predict\"):\n",
        "    X = np.array(vals).reshape(1,-1)\n",
        "    Xs = scaler.transform(X)\n",
        "    pred = model.predict(Xs)[0]\n",
        "    st.write(\"Predicted next-day 7-day rolling volatility:\", pred)\n",
        "'''\n",
        "with open('streamlit_app_stub.py', 'w') as f:\n",
        "    f.write(streamlit_app)\n",
        "print(\"\\nStreamlit app stub saved to streamlit_app_stub.py (run locally after downloading models).\")\n",
        "\n",
        "# ----\n",
        "# 12. Evaluation report notes (what to include in your submission)\n",
        "# ----\n",
        "# Include:\n",
        "# - Data cleaning steps and rationale (how missing values handled)\n",
        "# - Feature engineering list and motivation (why rolling vol, ATR, Bollinger width, liquidity)\n",
        "# - Model selection choices and baseline vs tuned results (RMSE, MAE, R2, accuracy/precision/recall for classification)\n",
        "# - Plots: time-series price, volatility time-series, feature importances, confusion matrix\n",
        "# - HLD & LLD bullets:\n",
        "#   HLD: Data ingestion -> Preprocessing -> Feature Engineering -> Model Training -> Model Serving (Streamlit/Flask)\n",
        "#   LLD: scripts for extract, transform, features (e.g., preprocess.py), training (train.py), inference (predict.py), app (streamlit app)\n",
        "#\n",
        "# ----\n",
        "# 13. Next steps & enhancements\n",
        "# ----\n",
        "# - Use more advanced models (XGBoost/LightGBM/CatBoost) and compare\n",
        "# - Use time-series specific models: ARIMA/GARCH for volatility, LSTM/Transformer for sequence modeling\n",
        "# - Use walk-forward validation instead of random CV for time-series robustness\n",
        "# - Calibrate classification thresholds using business costs (false negative vs false positive)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "QtsD01NoJ9XB",
        "outputId": "863b40a7-f788-4f38-9259-3bd188ba672a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "/mnt/data/dataset.csv.zip not found. Upload dataset.csv.zip to /mnt/data/",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1782281972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mzip_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/mnt/data/dataset.csv.zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{zip_path} not found. Upload dataset.csv.zip to /mnt/data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: /mnt/data/dataset.csv.zip not found. Upload dataset.csv.zip to /mnt/data/"
          ]
        }
      ]
    }
  ]
}