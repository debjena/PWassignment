{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SVM & Naive Bayes**"
      ],
      "metadata": {
        "id": "tKSd6k3XH55h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Q1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "# Answer:\n",
        "# SVM is a supervised learning algorithm that finds the optimal hyperplane separating classes\n",
        "# with the maximum margin. Only the training points on the edges of the margin (support vectors)\n",
        "# influence the decision boundary. For non-linearly separable data, SVM uses kernels to project\n",
        "# data into higher-dimensional feature spaces where a linear separator can exist.\n",
        "\n",
        "# ✅ Q2: Difference between Hard Margin and Soft Margin SVM\n",
        "# Answer:\n",
        "# - Hard Margin: Assumes data are perfectly linearly separable; no misclassification allowed.\n",
        "#   Maximizes margin with strict constraints (prone to overfitting/outliers).\n",
        "# - Soft Margin: Allows some violations controlled by C (regularization). Balances margin size\n",
        "#   and classification errors; robust to outliers and noise.\n",
        "\n",
        "# ✅ Q3: Kernel Trick in SVM + Example\n",
        "# Answer:\n",
        "# The kernel trick computes inner products in a high-dimensional (possibly infinite) feature space\n",
        "# without explicitly transforming the data. Example: RBF (Gaussian) kernel k(x, x') = exp(-γ||x - x'||²).\n",
        "# Use case: When classes are not linearly separable in the original space; RBF flexibly models complex, curved boundaries.\n",
        "\n",
        "# ✅ Q4: What is a Naïve Bayes Classifier, and why “naïve”?\n",
        "# Answer:\n",
        "# Naïve Bayes applies Bayes’ Theorem with the assumption that features are conditionally independent\n",
        "# given the class label — that simplifying assumption is the “naïve” part. It is fast, works well with high-dimensional,\n",
        "# sparse text data, and yields calibrated probabilities.\n",
        "\n",
        "# ✅ Q5: Gaussian, Multinomial, and Bernoulli Naïve Bayes — when to use which?\n",
        "# Answer:\n",
        "# - Gaussian NB: Continuous, approximately normal features (e.g., sensor readings).\n",
        "# - Multinomial NB: Count features (e.g., word counts in documents via CountVectorizer/TF-IDF).\n",
        "# - Bernoulli NB: Binary features (presence/absence of words, or thresholded features).\n",
        "\n",
        "# =========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# -------------------------\n",
        "# Q6: Iris + SVM (linear kernel): print accuracy & support vectors\n",
        "# -------------------------\n",
        "print(\"\\n\" + \"=\"*15 + \" Q6: SVM (Linear) on Iris \" + \"=\"*15)\n",
        "iris = datasets.load_iris()\n",
        "X_i, y_i = iris.data, iris.target\n",
        "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_i, y_i, test_size=0.3, random_state=42, stratify=y_i)\n",
        "\n",
        "# Without scaling (linear SVM can still benefit from scaling; we show both as asked)\n",
        "svm_linear_noscale = SVC(kernel='linear', random_state=42)\n",
        "svm_linear_noscale.fit(X_train_i, y_train_i)\n",
        "y_pred_i_ns = svm_linear_noscale.predict(X_test_i)\n",
        "acc_i_ns = accuracy_score(y_test_i, y_pred_i_ns)\n",
        "\n",
        "# With scaling\n",
        "pipe_linear_scaled = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\", SVC(kernel='linear', random_state=42))\n",
        "])\n",
        "pipe_linear_scaled.fit(X_train_i, y_train_i)\n",
        "y_pred_i_sc = pipe_linear_scaled.predict(X_test_i)\n",
        "acc_i_sc = accuracy_score(y_test_i, y_pred_i_sc)\n",
        "\n",
        "print(f\"Accuracy (no scaling): {acc_i_ns:.4f}\")\n",
        "print(f\"Accuracy (with scaling): {acc_i_sc:.4f}\")\n",
        "\n",
        "# Support vectors are only accessible from the fitted SVC (not from pipeline directly)\n",
        "n_support = svm_linear_noscale.n_support_\n",
        "svectors_shape = svm_linear_noscale.support_vectors_.shape\n",
        "print(f\"Support vectors per class (no scaling model): {n_support}\")\n",
        "print(f\"Support vectors array shape (no scaling model): {svectors_shape}\")\n",
        "\n",
        "# -------------------------\n",
        "# Q7: Breast Cancer + Gaussian Naive Bayes — classification report\n",
        "# -------------------------\n",
        "print(\"\\n\" + \"=\"*15 + \" Q7: GaussianNB on Breast Cancer \" + \"=\"*15)\n",
        "bc = datasets.load_breast_cancer()\n",
        "X_b, y_b = bc.data, bc.target\n",
        "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.3, random_state=42, stratify=y_b)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_b, y_train_b)\n",
        "y_pred_b = gnb.predict(X_test_b)\n",
        "print(classification_report(y_test_b, y_pred_b, target_names=bc.target_names))\n",
        "\n",
        "# -------------------------\n",
        "# Q8: Wine + SVM with GridSearchCV (tune C, gamma for RBF)\n",
        "# -------------------------\n",
        "print(\"\\n\" + \"=\"*15 + \" Q8: GridSearch SVM (RBF) on Wine \" + \"=\"*15)\n",
        "wine = datasets.load_wine()\n",
        "X_w, y_w = wine.data, wine.target\n",
        "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size=0.3, random_state=42, stratify=y_w)\n",
        "\n",
        "pipe_rbf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\", SVC(kernel='rbf', probability=False, random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"svm__C\": [0.1, 1, 10, 100],\n",
        "    \"svm__gamma\": [\"scale\", 0.1, 0.01, 0.001]\n",
        "}\n",
        "grid = GridSearchCV(pipe_rbf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid.fit(X_train_w, y_train_w)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "acc_w = accuracy_score(y_test_w, best_model.predict(X_test_w))\n",
        "print(f\"Best Params: {grid.best_params_}\")\n",
        "print(f\"CV Best Score: {grid.best_score_:.4f}\")\n",
        "print(f\"Test Accuracy with Best Params: {acc_w:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Q9: Naive Bayes on text (20 Newsgroups subset) + ROC-AUC\n",
        "# -------------------------\n",
        "print(\"\\n\" + \"=\"*15 + \" Q9: Text NB + ROC-AUC \" + \"=\"*15)\n",
        "# We'll use a binary subset for a clean ROC-AUC (spam-like vs ham-like themes).\n",
        "# If fetching fails (no internet), we fall back to a small synthetic example.\n",
        "try:\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    cats = ['sci.space', 'talk.politics.mideast']  # two classes -> binary ROC-AUC\n",
        "    train = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "    test  = fetch_20newsgroups(subset='test',  categories=cats, remove=('headers','footers','quotes'))\n",
        "    X_tr_text, y_tr = train.data, train.target\n",
        "    X_te_text, y_te = test.data,  test.target\n",
        "\n",
        "    text_pipe = Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer(min_df=3, ngram_range=(1,2))),\n",
        "        (\"nb\", MultinomialNB())\n",
        "    ])\n",
        "    text_pipe.fit(X_tr_text, y_tr)\n",
        "    # ROC-AUC for binary: use predict_proba or decision_function\n",
        "    proba = text_pipe.predict_proba(X_te_text)[:, 1]\n",
        "    auc = roc_auc_score(y_te, proba)\n",
        "    print(f\"ROC-AUC (MultinomialNB on 20 Newsgroups subset): {auc:.4f}\")\n",
        "except Exception as e:\n",
        "    print(\"Fetch failed, using synthetic tiny text dataset. Reason:\", str(e))\n",
        "    X_text = [\n",
        "        \"earn money fast limited offer buy now\",\n",
        "        \"investment opportunity returns guaranteed\",\n",
        "        \"meeting schedule for next week and agenda\",\n",
        "        \"project update attached please review\",\n",
        "        \"cheap meds available without prescription\",\n",
        "        \"family dinner plan this weekend at home\"\n",
        "    ]\n",
        "    y_text = np.array([1,1,0,0,1,0])  # 1=spam-like, 0=ham\n",
        "    X_tr_text, X_te_text, y_tr, y_te = train_test_split(X_text, y_text, test_size=0.5, random_state=42, stratify=y_text)\n",
        "    text_pipe = Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer()),\n",
        "        (\"nb\", MultinomialNB())\n",
        "    ])\n",
        "    text_pipe.fit(X_tr_text, y_tr)\n",
        "    proba = text_pipe.predict_proba(X_te_text)[:, 1]\n",
        "    auc = roc_auc_score(y_te, proba)\n",
        "    print(f\"ROC-AUC (MultinomialNB on synthetic text): {auc:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# Q10: Spam vs Not-Spam — Full Approach (Theory + Practical Guidance)\n",
        "# =========================\n",
        "# Scenario:\n",
        "# - Text emails with diverse vocabulary\n",
        "# - Class imbalance (ham >> spam)\n",
        "# - Missing/incomplete data\n",
        "\n",
        "# Approach:\n",
        "# 1) Preprocessing\n",
        "#    - Handle missing: replace missing subject/body with empty string; drop rows with no usable text if needed.\n",
        "#    - Text cleaning: lowercasing, optional punctuation/URL removal (preserve numbers if useful).\n",
        "#    - Vectorization: TfidfVectorizer with n-grams (1–2), min_df to remove rare noise, optionally sublinear_tf=True.\n",
        "#    - Optional feature selection: chi-square to keep top-k features for speed.\n",
        "#\n",
        "# 2) Model choice & justification (SVM vs Naive Bayes)\n",
        "#    - MultinomialNB:\n",
        "#        * Pros: Very fast, robust with high-dimensional sparse text, good baseline, calibrated probabilities.\n",
        "#        * Cons: Assumes feature independence; decision boundary is simpler.\n",
        "#    - Linear SVM (e.g., LinearSVC or SVC(kernel='linear')):\n",
        "#        * Pros: Strong performance on text, handles high-dimensional sparse data well.\n",
        "#        * Cons: Needs probability calibration (CalibratedClassifierCV) if you need probabilities.\n",
        "#    - Recommendation: Start with MultinomialNB as baseline, then train Linear SVM; pick the better one via CV.\n",
        "#\n",
        "# 3) Address class imbalance\n",
        "#    - Use class_weight='balanced' for SVM; for NB, adjust decision threshold post-hoc.\n",
        "#    - Resampling strategies: RandomUnderSampler/SMOTE (on TF-IDF can be tricky; usually adjust threshold + class weights first).\n",
        "#    - Use stratified folds in cross-validation.\n",
        "#\n",
        "# 4) Evaluation metrics\n",
        "#    - Accuracy can be misleading with imbalance; focus on Precision, Recall, F1 (particularly Recall for spam),\n",
        "#      PR-AUC (Precision-Recall AUC), ROC-AUC.\n",
        "#    - Calibrate threshold to meet business objectives (e.g., minimize false negatives or false positives depending on cost).\n",
        "#\n",
        "# 5) Business impact\n",
        "#    - Reduced exposure to spam & phishing, improved employee productivity.\n",
        "#    - Cost control by reducing manual triage; explainable thresholds for compliance.\n",
        "#    - Continuous monitoring: track drift (new spam tactics) and retrain periodically.\n",
        "\n",
        "# (Optional) Mini reference pipeline sketch (no execution, just for clarity):\n",
        "# from sklearn.calibration import CalibratedClassifierCV\n",
        "# pipe = Pipeline([\n",
        "#     (\"tfidf\", TfidfVectorizer(min_df=3, ngram_range=(1,2), sublinear_tf=True)),\n",
        "#     (\"clf\", LinearSVC(class_weight=\"balanced\", random_state=42))\n",
        "# ])\n",
        "# clf = CalibratedClassifierCV(pipe, method=\"sigmoid\", cv=5)  # to get probabilities if needed\n",
        "# clf.fit(X_train_text, y_train)\n",
        "# y_proba = clf.predict_proba(X_test_text)[:,1]\n",
        "# Evaluate: precision, recall, f1, roc_auc, pr_auc; choose threshold by business cost curve.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uILSuta7H9xv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}